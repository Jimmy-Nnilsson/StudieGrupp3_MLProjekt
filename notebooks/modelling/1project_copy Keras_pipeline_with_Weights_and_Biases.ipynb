{"cells":[{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TF:  2.7.0\n"]}],"source":["import os\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import tensorflow as tf\n","print(\"TF: \", tf.__version__)\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from keras import Model\n","\n","from pathlib import Path\n","from keras.preprocessing.image import load_img, img_to_array, image_dataset_from_directory\n","from tensorflow.keras.applications import vgg16, vgg19\n","\n","import wandb\n","from wandb.keras import WandbCallback"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def seed_everything():\n","    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n","    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n","    tf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n","\n","seed_everything()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","# Get base project directory\n","project_path = Path(os.getcwd()).parent.parent\n","datapath = (project_path /'data/processed/')\n","\n","CLASSES = {0 : 'yes', 1 : 'no'}\n","# Loops through pathlist and reads and resizes images\n","def read_image(pathlist : list, size : int)-> list:\n","    data = []\n","    for path in pathlist:\n","        image=load_img(path, color_mode='rgb', target_size=(size, size))\n","        image=img_to_array(image)\n","        # image=image/255.0\n","        data.append(image)\n","    return data\n","\n","# Makes input and label data from folder locations.\n","# Loops through location \"subfolder/CLASSES\"\n","def get_sets(subfolder : str, CLASSES : dict, size : int) -> tuple[list, list]:\n","    folder_paths = []\n","    folder_labels = []\n","    labels = []\n","    for k,v in CLASSES.items():\n","        # input datapath generation\n","        folder_paths += list((datapath / f\"2_split_{v}/{subfolder}\").rglob(\"*\"))\n","    # Label data generation\n","    folder_labels = [0 if x.stem.split('_')[1] == 'yes' else 1 for x in folder_paths]\n","    # Extract images from datapaths\n","    img_list = read_image(folder_paths, size)\n","\n","    return img_list, folder_labels\n","\n","def get_training_set(CLASSES : dict, size : int) -> tuple[list, list]:\n","    folder_paths = []\n","    folder_labels = []\n","    labels = []\n","    for k,v in CLASSES.items():\n","        # input datapath generation\n","        folder_paths += list((datapath / f\"3_aug_{v}_train/\").rglob(\"*\"))\n","        # print(folder_paths)\n","    # Label data generation\n","    folder_labels = [0 if x.stem.split('_')[1] == 'yes' else 1 for x in folder_paths]\n","    # Extract images from datapaths\n","    img_list = read_image(folder_paths, size)\n","\n","    return img_list, folder_labels"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Dataset inspect\n","# Read images to variables\n","size = 224\n","# X_train, y_train = get_training_set(CLASSES, size)\n","X_train, y_train = get_sets('train', CLASSES, size)\n","X_val, y_val = get_sets('val', CLASSES, size)\n","X_test, y_test = get_sets('test', CLASSES, size)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"KHhaOEzCI-w0"},"outputs":[],"source":["configs = dict(\n","    image_width = X_train[0].shape[0],\n","    image_height = X_train[0].shape[1],\n","    image_channels = X_train[0].shape[2],\n","    batch_size = 32,\n","    class_names = CLASSES,\n","    model_name = '', # set after model is defined\n","    pretrain_weights = 'imagenet',\n","    epochs = 5,\n","    init_learning_rate = 0.001,\n","    lr_decay_rate = 0.1,\n","    optimizer = 'adam',\n","    loss_fn = 'binary_crossentropy',\n","    metrics = ['accuracy'],\n","    earlystopping_patience = 5,\n","    architecture = \"\",# To be defined f\"{base_model._name.upper()} global_average_pooling2d\",\n","    dataset = \"Brain MRI Images for Brain Tumor Detection\"\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"uAnYG27BI-w2"},"outputs":[],"source":["# train_images, train_labels, valid_images, valid_labels, test_images, test_labels = download_and_prepare_dataset(info)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"pCaZpItDI-w2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of train images : 50 to be logged\n"]}],"source":["# For demonstration purposes\n","log_full = False #@param {type:\"boolean\"}\n","\n","if log_full:\n","    log_train_samples = len(X_train)\n","else:\n","    log_train_samples = 50 \n","\n","print(f'Number of train images : {log_train_samples} to be logged')"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Or7MpNbkI-w3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbex_team\u001b[0m (use `wandb login --relogin` to force relogin)\n","C:\\Users\\Jimmy\\anaconda3\\envs\\g3\\lib\\site-packages\\IPython\\html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n","  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"]},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/bex_team/baseline_vgg19/runs/ir2g0rpa\" target=\"_blank\">brisk-dust-19</a></strong> to <a href=\"https://wandb.ai/bex_team/baseline_vgg19\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyError","evalue":"'0'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\g3\\lib\\site-packages\\wandb\\data_types.py\u001b[0m in \u001b[0;36madd_computed_columns\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mndx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[0mrow_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0mnew_row_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_row_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_row_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(ndx, row)\u001b[0m\n","\u001b[1;31mKeyError\u001b[0m: '0'"]}],"source":["%%time\n","\n","# Initialize a new W&B run\n","run = wandb.init(project=\"baseline_vgg19\",\n","                 entity=\"bex_team\",\n","                 group=\"Pictures\",\n","                 job_type=\"Baseline\")\n","\n","# Intialize a W&B Artifacts\n","ds = wandb.Artifact(\"medmnist_bloodmnist_dataset\", \"dataset\")\n","\n","# Initialize an empty table\n","train_table = wandb.Table(columns=[], data=[])\n","# Add training data\n","train_table.add_column('image', X_train[:log_train_samples])\n","# Add training label_id\n","train_table.add_column('label_id', y_train[:log_train_samples])\n","# Add training class names\n","train_table.add_computed_columns(lambda ndx, row:{\n","    \"images\": wandb.Image(row[\"image\"]),\n","    \"class_names\": configs['class_names'][str(row[\"label_id\"])]\n","    })\n","\n","# Add the table to the Artifact\n","ds['train_data'] = train_table\n","\n","# Let's do the same for the validation data\n","valid_table = wandb.Table(columns=[], data=[])\n","valid_table.add_column('image', X_val)\n","valid_table.add_column('label_id', y_val)\n","valid_table.add_computed_columns(lambda ndx, row:{\n","    \"images\": wandb.Image(row[\"image\"]),\n","    \"class_name\": configs['class_names'][str(row[\"label_id\"])]\n","    })\n","ds['valid_data'] = valid_table\n","\n","# Save the dataset as an Artifact\n","ds.save()\n","\n","# Finish the run\n","wandb.finish()"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"wmjJlRYqI-w6"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Model was constructed with shape (None, 32, 32, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 224, 224, 3).\n","Model: \"Baseline_VGG19\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," tf.__operators__.getitem (S  (None, 224, 224, 3)      0         \n"," licingOpLambda)                                                 \n","                                                                 \n"," tf.nn.bias_add (TFOpLambda)  (None, 224, 224, 3)      0         \n","                                                                 \n"," vgg19 (Functional)          (None, 1, 1, 512)         20024384  \n","                                                                 \n"," global_average_pooling2d (G  (None, 512)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dense (Dense)               (None, 1)                 513       \n","                                                                 \n","=================================================================\n","Total params: 20,024,897\n","Trainable params: 513\n","Non-trainable params: 20,024,384\n","_________________________________________________________________\n"]}],"source":["def get_model(input_shape: tuple=(32, 32, 3),\n","              output_activation: str='sigmoid'):\n","\n","    inputs = layers.Input(input_shape)\n","\n","    base_model = vgg19.VGG19(weights=configs['pretrain_weights'], include_top=False, input_shape=input_shape)\n","    base_model.trainable = False\n","    configs['architecture'] = f\"{base_model._name.upper()} global_average_pooling2d\"\n","\n","    inputs = layers.Input(shape=(X_train[0].shape[0], X_train[0].shape[1], 3))\n","    x = vgg19.preprocess_input(inputs)\n","    x = base_model(x)\n","    x = layers.GlobalAveragePooling2D()(x)\n","    # x = Flatten()(x)\n","    outputs = layers.Dense(1, activation=output_activation)(x)\n","\n","    return models.Model(inputs, outputs, name=f'Baseline_{base_model._name.upper()}')\n","\n","tf.keras.backend.clear_session()\n","model = get_model()\n","model.summary()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["checkpoint_filepath = (Path(os.getcwd()) /'model_checkpoint/model_checkpoint')\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"XUld9o-zI-w6"},"outputs":[],"source":["earlystopper = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', patience=configs['earlystopping_patience'], verbose=0, mode='auto',\n","    restore_best_weights=True\n",")"]},{"cell_type":"markdown","metadata":{"id":"FCd5Ao88I-w6"},"source":["You can use `wandb.log` to log any useful metric/parameter that's not logged by `WandbCallback`. Here we are using a learning rate scheduler to exponentially decay the learning rate after 10 epochs. Notice the use of `wandb.log` to capture the learning rate and `commit=False` in particular.\n","\n","You can learn more about `wandb.log` [here](https://docs.wandb.ai/guides/track/log).\n","\n","def lr_scheduler(epoch, lr):\n","    # log the current learning rate onto W&B\n","    if wandb.run is None:\n","        raise wandb.Error(\"You must call wandb.init() before WandbCallback()\")\n","\n","    wandb.log({'learning_rate': lr}, commit=False)\n","    \n","    if epoch < 7:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-configs['lr_decay_rate'])\n","\n","lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"kORq8cZJI-w7"},"outputs":[],"source":["def train(config: dict,\n","          callbacks: list,\n","          verbose: int=0):\n","    \"\"\"\n","    Utility function to train the model.\n","\n","    Arguments:\n","        config (dict): Dictionary of hyperparameters.\n","        callbacks (list): List of callbacks passed to `model.fit`.\n","        verbose (int): 0 for silent and 1 for progress bar.\n","    \"\"\"\n","\n","    # Initalize model\n","    tf.keras.backend.clear_session()\n","    model = get_model(input_shape=(config.image_width, config.image_height, config.image_channels))\n","    config['model_name'] = model.name # set\n","\n","\n","    # Compile the model\n","    opt = tf.keras.optimizers.Adam(learning_rate=config.init_learning_rate)\n","    model.compile(optimizer=opt,\n","                  loss=config.loss_fn,\n","                  metrics=config.metrics)\n","# model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=metrics)\n","    # Train the model\n","    _ = model.fit(np.array(X_train), np.array(y_train),\n","                  epochs=config.epochs,\n","                  validation_data=(np.array(X_val),np.array(y_val)),\n","                  callbacks=callbacks,\n","                  batch_size=config.batch_size,\n","                  verbose=verbose)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbrdw5KoI-w7"},"outputs":[],"source":["# Initialize the W&B run\n","run = wandb.init(project='test_wandb', config=configs, job_type='Baseline')\n","config = wandb.config\n","\n","# Define WandbCallback for experiment tracking\n","wandb_callback = WandbCallback(monitor='val_loss',\n","                               log_weights=True,\n","                               log_evaluation=True,\n","                               validation_steps=5,\n","                               )\n","# WandbCallback(data_type='image', training_data=(np.array(X_val),np.array(y_val)), labels=CLASSES, save_model=True, save_graph=True)\n","# callbacks\n","callbacks = [earlystopper, wandb_callback]#lr_callback\n","\n","# Train\n","model = train(config, callbacks=callbacks, verbose=2)\n","\n","# Evaluate the trained model\n","loss, acc = model.evaluate(np.array(X_val),np.array(y_val))\n","wandb.log({'evaluate/accuracy': acc})\n","\n","# Close the W&B run.\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJU_xbWCI-w8"},"outputs":[],"source":["#@title\n","def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n","    # First, we create a model that maps the input image to the activations\n","    # of the last conv layer as well as the output predictions\n","    grad_model = tf.keras.models.Model(\n","        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n","    )\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = grad_model(img_array)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","\n","    # This is the gradient of the output neuron (top predicted or chosen)\n","    # with regard to the output feature map of the last conv layer\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the feature map array\n","    # by \"how important this channel is\" with regard to the top predicted class\n","    # then sum all the channels to obtain the heatmap class activation\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","    return heatmap.numpy()\n","\n","def create_gradcam(image, model, last_conv_layer_name, pred_index=None):\n","    # Preprocess the image array\n","    image, _ = preprocess(tf.expand_dims(image, axis=0), 0)\n","    # Get GradCAM\n","    heatmap = make_gradcam_heatmap(image, model, last_conv_layer_name, pred_index)\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # Use jet colormap to colorize heatmap\n","    jet = cm.get_cmap(\"jet\")\n","\n","    # Use RGB values of the colormap\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","    jet_heatmap = tf.image.resize(jet_heatmap, size=(28,28))\n","\n","    # Overlay\n","    superimposed_img = jet_heatmap * 0.4 + tf.squeeze(image, axis=0)\n","    superimposed_img = tf.clip_by_value(superimposed_img, 0.0, 1.0)\n","\n","    return superimposed_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yUye2ChI-w8"},"outputs":[],"source":["last_conv_layer_name = 'block4_conv3'"]},{"cell_type":"markdown","metadata":{"id":"m-cpGKdOI-w8"},"source":["In the cell block below, we will be using `WandbCallback`'s `validation_row_processor` and `prediction_row_processor` to log the images, ground truth label, model prediction and the GradCAM for model interpretability. \n","\n","The processors' take a callable function that receive an `ndx` (index) and a `row` (dict of data). The `validation_processor` function below receives the input image array along with target label as `row` dict. The `prediction_processor` receives  the model output prediction and the validation data row index. \n","\n","The `validation_row_processor` is executed when `WandbCallback` is initialized (i.e, before model training) while `prediction_row_processor` is called once the training is over. The `validation_row_processor` creates a table with two columns `input:image` and `target:class`. Notice that in the `prediction_processor` function we can get the logged image at a given `val_row` using the `get_row` method. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25T7JVt4I-w8"},"outputs":[],"source":["def validation_processor(ndx, row):\n","    return {\n","        \"input:image\": wandb.Image(row[\"input\"]),\n","        \"target:class\": class_table.index_ref(row[\"target\"])\n","    }\n","\n","def prediction_processor(ndx, row):\n","    # Get the validation image\n","    valid_image = np.array(row[\"val_row\"].get_row()[\"input:image\"].image)\n","\n","    return {\n","        \"output:class\": class_table.index_ref(np.argmax(row[\"output\"])),\n","        \"gradcam\": wandb.Image(create_gradcam(valid_image, model, last_conv_layer_name)),\n","        \"output:logits\": {class_name: value for (class_name, value) in zip(list(config.class_names.values()), row[\"output\"].tolist())}\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiJJ9HjCI-w8"},"outputs":[],"source":["# Initialize the W&B run\n","run = wandb.init(project='medmnist-bloodmnist', config=configs, job_type='train')\n","config = wandb.config\n","\n","# Get validation table\n","data_art = run.use_artifact('ayush-thakur/medmnist-bloodmnist/medmnist_bloodmnist_dataset:latest', type='dataset')\n","valid_table = data_art.get(\"valid_data\")\n","\n","# Create a class table\n","class_table = wandb.Table(columns=[], data=[])\n","class_table.add_column(\"class_name\", list(config.class_names.values()))\n","\n","# Define WandbCallback for experiment tracking\n","wandb_callback = WandbCallback(\n","                    log_evaluation=True,\n","                    validation_row_processor=lambda ndx, row: validation_processor(ndx, row),\n","                    prediction_row_processor=lambda ndx, row: prediction_processor(ndx, row),\n","                    validation_steps=4,\n","                    save_model=False\n","                )\n","\n","# callbacks\n","callbacks = [earlystopper, wandb_callback]#lr_callback\n","\n","# Train\n","model = train(config, callbacks=callbacks, verbose=2)\n","\n","# Evaluate the trained model\n","loss, acc = model.evaluate(np.array(X_val),np.array(y_val))\n","wandb.log({'evaluate/accuracy': acc})\n","\n","# Close the W&B run.\n","wandb.finish()"]}],"metadata":{"colab":{"name":"Copy of Keras_pipeline_with_Weights_and_Biases.ipynb","provenance":[{"file_id":"https://github.com/wandb/examples/blob/master/colabs/keras/Keras_pipeline_with_Weights_and_Biases.ipynb","timestamp":1645008414025}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}
