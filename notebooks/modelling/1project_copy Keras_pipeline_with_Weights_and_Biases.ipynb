{"cells":[{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TF:  2.7.0\n"]}],"source":["import os\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import tensorflow as tf\n","print(\"TF: \", tf.__version__)\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from keras import Model\n","\n","from pathlib import Path\n","from keras.preprocessing.image import load_img, img_to_array, image_dataset_from_directory\n","from tensorflow.keras.applications import vgg16, vgg19\n","\n","import wandb\n","from wandb.keras import WandbCallback"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def seed_everything():\n","    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n","    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n","    tf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n","\n","seed_everything()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["#@title\n","@tf.function\n","def preprocess(image: tf.Tensor, label: tf.Tensor):\n","    \"\"\"\n","    Preprocess the image tensors and parse the labels\n","    \"\"\"\n","    # Preprocess images\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","    \n","    # Parse label\n","    label = tf.cast(label, tf.float32)\n","    \n","    return image, label"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["\n","# Get base project directory\n","project_path = Path(os.getcwd()).parent.parent\n","datapath = (project_path /'data/processed/')\n","\n","CLASSES = {0 : 'yes', 1 : 'no'}\n","# Loops through pathlist and reads and resizes images\n","def read_image(pathlist : list, size : int)-> list:\n","    data = []\n","    for path in pathlist:\n","        image=load_img(path, color_mode='rgb', target_size=(size, size))\n","        image=img_to_array(image)\n","        # image=image/255.0\n","        data.append(image)\n","    return data\n","\n","# Makes input and label data from folder locations.\n","# Loops through location \"subfolder/CLASSES\"\n","def get_sets(subfolder : str, CLASSES : dict, size : int) -> tuple[list, list]:\n","    folder_paths = []\n","    folder_labels = []\n","    labels = []\n","    for k,v in CLASSES.items():\n","        # input datapath generation\n","        folder_paths += list((datapath / f\"2_split_{v}/{subfolder}\").rglob(\"*\"))\n","    # Label data generation\n","    folder_labels = np.asarray([0 if x.stem.split('_')[1] == 'yes' else 1 for x in folder_paths],dtype=np.uint8)\n","    # Extract images from datapaths\n","    img_list = np.asarray(read_image(folder_paths, size))\n","\n","    return img_list, folder_labels\n","\n","def get_training_set(CLASSES : dict, size : int) -> tuple[list, list]:\n","    folder_paths = []\n","    folder_labels = []\n","    labels = []\n","    for k,v in CLASSES.items():\n","        # input datapath generation\n","        folder_paths += list((datapath / f\"3_aug_{v}_train/\").rglob(\"*\"))\n","        # print(folder_paths)\n","    # Label data generation\n","    folder_labels = [0 if x.stem.split('_')[1] == 'yes' else 1 for x in folder_paths]\n","    # Extract images from datapaths\n","    img_list = read_image(folder_paths, size)\n","\n","    return img_list, folder_labels"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Dataset inspect\n","# Read images to variables\n","size = 224\n","# X_train, y_train = get_training_set(CLASSES, size)\n","X_train, y_train = get_sets('train', CLASSES, size)\n","X_val, y_val = get_sets('val', CLASSES, size)\n","X_test, y_test = get_sets('test', CLASSES, size)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["project_config = dict(project_name = \"test_wandb\", image_group= \"viz_data\", entity='bex_team')"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"KHhaOEzCI-w0"},"outputs":[],"source":["configs = dict(\n","    image_width = X_train[0].shape[0],\n","    image_height = X_train[0].shape[1],\n","    image_channels = X_train[0].shape[2],\n","    batch_size = 32,\n","    class_names = CLASSES,\n","    model_name = '', # set after model is defined\n","    pretrain_weights = 'imagenet',\n","    epochs = 5,\n","    init_learning_rate = 0.001,\n","    lr_decay_rate = 0.1,\n","    optimizer = 'adam',\n","    loss_fn = 'binary_crossentropy',\n","    metrics = ['accuracy'],\n","    earlystopping_patience = 5,\n","    architecture = \"\",# To be defined f\"{base_model._name.upper()} global_average_pooling2d\",\n","    dataset = \"Brain_MRI_Images_for_Brain_Tumor_Detection\"\n",")"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"pCaZpItDI-w2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of train images : 125 to be logged\n"]}],"source":["# For demonstration purposes\n","log_full = True #@param {type:\"boolean\"}\n","\n","if log_full:\n","    log_train_samples = len(X_train)\n","else:\n","    log_train_samples = 50 \n","\n","print(f'Number of train images : {log_train_samples} to be logged')"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"Or7MpNbkI-w3"},"outputs":[{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/bex_team/test_wandb/runs/1nk6dmv0\" target=\"_blank\">restful-microwave-1</a></strong> to <a href=\"https://wandb.ai/bex_team/test_wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<br/>Waiting for W&B process to finish, PID 7980... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","</div><div class=\"wandb-col\">\n","</div></div>\n","Synced 5 W&B file(s), 0 media file(s), 171 artifact file(s) and 0 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">restful-microwave-1</strong>: <a href=\"https://wandb.ai/bex_team/test_wandb/runs/1nk6dmv0\" target=\"_blank\">https://wandb.ai/bex_team/test_wandb/runs/1nk6dmv0</a><br/>\n","Find logs at: <code>.\\wandb\\run-20220217_121237-1nk6dmv0\\logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Wall time: 31.2 s\n"]}],"source":["%%time\n","\n","# Initialize a new W&B run\n","run = wandb.init(project=project_config['project_name'],\n","                 entity=project_config['entity'],\n","                 group=project_config['image_group'])\n","\n","# Intialize a W&B Artifacts\n","ds = wandb.Artifact(configs['dataset'], \"dataset\")\n","\n","# Initialize an empty table\n","train_table = wandb.Table(columns=[], data=[])\n","# Add training data\n","train_table.add_column('image', X_train[:log_train_samples])\n","# Add training label_id\n","train_table.add_column('label_id', y_train[:log_train_samples])\n","# Add training class names\n","train_table.add_computed_columns(lambda ndx, row:{\n","    \"images\": wandb.Image(row[\"image\"]),\n","    \"class_names\": configs['class_names'][row[\"label_id\"]]\n","    })\n","\n","# Add the table to the Artifact\n","ds['train_data'] = train_table\n","\n","# Let's do the same for the validation data\n","valid_table = wandb.Table(columns=[], data=[])\n","valid_table.add_column('image', X_val)\n","valid_table.add_column('label_id', y_val)\n","valid_table.add_computed_columns(lambda ndx, row:{\n","    \"images\": wandb.Image(row[\"image\"]),\n","    \"class_name\": configs['class_names'][row[\"label_id\"]]\n","    })\n","ds['valid_data'] = valid_table\n","\n","# Save the dataset as an Artifact\n","ds.save()\n","\n","# Finish the run\n","wandb.finish()"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"wmjJlRYqI-w6"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Model was constructed with shape (None, 32, 32, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 224, 224, 3).\n","Model: \"Baseline_VGG19\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," tf.__operators__.getitem (S  (None, 224, 224, 3)      0         \n"," licingOpLambda)                                                 \n","                                                                 \n"," tf.nn.bias_add (TFOpLambda)  (None, 224, 224, 3)      0         \n","                                                                 \n"," vgg19 (Functional)          (None, 1, 1, 512)         20024384  \n","                                                                 \n"," global_average_pooling2d (G  (None, 512)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dense (Dense)               (None, 1)                 513       \n","                                                                 \n","=================================================================\n","Total params: 20,024,897\n","Trainable params: 513\n","Non-trainable params: 20,024,384\n","_________________________________________________________________\n"]}],"source":["def get_model(input_shape: tuple=(32, 32, 3),\n","              output_activation: str='sigmoid'):\n","\n","    inputs = layers.Input(input_shape)\n","\n","    base_model = vgg19.VGG19(weights=configs['pretrain_weights'], include_top=False, input_shape=input_shape)\n","    base_model.trainable = False\n","    configs['architecture'] = base_model._name\n","\n","    inputs = layers.Input(shape=(X_train[0].shape[0], X_train[0].shape[1], 3))\n","    x = vgg19.preprocess_input(inputs)\n","    x = base_model(x)\n","    x = layers.GlobalAveragePooling2D()(x)\n","    # x = Flatten()(x)\n","    outputs = layers.Dense(1, activation=output_activation)(x)\n","\n","    return models.Model(inputs, outputs, name=f'Baseline_{base_model._name.upper()}')\n","\n","tf.keras.backend.clear_session()\n","model = get_model()\n","model.summary()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["checkpoint_filepath = (Path(os.getcwd()) /'model_checkpoint/model_checkpoint')\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"XUld9o-zI-w6"},"outputs":[],"source":["earlystopper = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', patience=configs['earlystopping_patience'], verbose=0, mode='auto',\n","    restore_best_weights=True\n",")"]},{"cell_type":"markdown","metadata":{"id":"FCd5Ao88I-w6"},"source":["You can use `wandb.log` to log any useful metric/parameter that's not logged by `WandbCallback`. Here we are using a learning rate scheduler to exponentially decay the learning rate after 10 epochs. Notice the use of `wandb.log` to capture the learning rate and `commit=False` in particular.\n","\n","You can learn more about `wandb.log` [here](https://docs.wandb.ai/guides/track/log).\n","\n","def lr_scheduler(epoch, lr):\n","    # log the current learning rate onto W&B\n","    if wandb.run is None:\n","        raise wandb.Error(\"You must call wandb.init() before WandbCallback()\")\n","\n","    wandb.log({'learning_rate': lr}, commit=False)\n","    \n","    if epoch < 7:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-configs['lr_decay_rate'])\n","\n","lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"kORq8cZJI-w7"},"outputs":[],"source":["def train(config: dict,\n","          callbacks: list,\n","          verbose: int=0):\n","    \"\"\"\n","    Utility function to train the model.\n","\n","    Arguments:\n","        config (dict): Dictionary of hyperparameters.\n","        callbacks (list): List of callbacks passed to `model.fit`.\n","        verbose (int): 0 for silent and 1 for progress bar.\n","    \"\"\"\n","\n","    # Initalize model\n","    tf.keras.backend.clear_session()\n","    model = get_model(input_shape=(config.image_width, config.image_height, config.image_channels))\n","    config['model_name'] = model.name # set\n","\n","\n","    # Compile the model\n","    opt = tf.keras.optimizers.Adam(learning_rate=config.init_learning_rate)\n","    model.compile(optimizer=opt,\n","                  loss=config.loss_fn,\n","                  metrics=config.metrics)\n","# model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=metrics)\n","    # Train the model\n","    _ = model.fit(np.array(X_train), np.array(y_train),\n","                  epochs=config.epochs,\n","                  validation_data=(np.array(X_val),np.array(y_val)),\n","                  callbacks=callbacks,\n","                  batch_size=config.batch_size,\n","                  verbose=verbose)\n","    return model"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"dbrdw5KoI-w7"},"outputs":[{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/bex_team/test_wandb/runs/3m2q1hav\" target=\"_blank\">balmy-silence-2</a></strong> to <a href=\"https://wandb.ai/bex_team/test_wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","4/4 - 3s - loss: 1.7969 - accuracy: 0.4720 - val_loss: 0.9113 - val_accuracy: 0.6429 - 3s/epoch - 630ms/step\n","Epoch 2/5\n","4/4 - 1s - loss: 1.0131 - accuracy: 0.5440 - val_loss: 0.9076 - val_accuracy: 0.6429 - 1s/epoch - 305ms/step\n","Epoch 3/5\n","4/4 - 1s - loss: 1.1074 - accuracy: 0.6480 - val_loss: 1.0269 - val_accuracy: 0.6429 - 767ms/epoch - 192ms/step\n","Epoch 4/5\n","4/4 - 1s - loss: 1.0461 - accuracy: 0.6320 - val_loss: 0.7988 - val_accuracy: 0.6190 - 1s/epoch - 282ms/step\n","Epoch 5/5\n","4/4 - 1s - loss: 0.8658 - accuracy: 0.6480 - val_loss: 0.6919 - val_accuracy: 0.6429 - 1s/epoch - 278ms/step\n","2/2 [==============================] - 0s 60ms/step - loss: 0.6919 - accuracy: 0.6429\n"]},{"data":{"text/html":["<br/>Waiting for W&B process to finish, PID 15132... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄█▇█</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>evaluate/accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▂▃▂▁</td></tr><tr><td>val_accuracy</td><td>███▁█</td></tr><tr><td>val_loss</td><td>▆▆█▃▁</td></tr></table><br/></div><div class=\"wandb-col\">\n","<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.648</td></tr><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.69195</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>evaluate/accuracy</td><td>0.64286</td></tr><tr><td>loss</td><td>0.86578</td></tr><tr><td>val_accuracy</td><td>0.64286</td></tr><tr><td>val_loss</td><td>0.69195</td></tr></table>\n","</div></div>\n","Synced 6 W&B file(s), 0 media file(s), 44 artifact file(s) and 1 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">balmy-silence-2</strong>: <a href=\"https://wandb.ai/bex_team/test_wandb/runs/3m2q1hav\" target=\"_blank\">https://wandb.ai/bex_team/test_wandb/runs/3m2q1hav</a><br/>\n","Find logs at: <code>.\\wandb\\run-20220217_121309-3m2q1hav\\logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Initialize the W&B run\n","run = wandb.init(project='test_wandb', config=configs, job_type='Baseline')\n","config = wandb.config\n","\n","# Define WandbCallback for experiment tracking\n","wandb_callback = WandbCallback(monitor='val_loss',\n","                               log_weights=True,\n","                               log_evaluation=True,\n","                               validation_steps=5,\n","                               )\n","# WandbCallback(data_type='image', training_data=(np.array(X_val),np.array(y_val)), labels=CLASSES, save_model=True, save_graph=True)\n","# callbacks\n","callbacks = [earlystopper, wandb_callback]#lr_callback\n","\n","# Train\n","model = train(config, callbacks=callbacks, verbose=2)\n","\n","# Evaluate the trained model\n","loss, acc = model.evaluate(np.array(X_val),np.array(y_val))\n","wandb.log({'evaluate/accuracy': acc})\n","\n","# Close the W&B run.\n","wandb.finish()"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"PJU_xbWCI-w8"},"outputs":[],"source":["#@title\n","def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n","    # First, we create a model that maps the input image to the activations\n","    # of the last conv layer as well as the output predictions\n","    grad_model = tf.keras.models.Model(\n","        [model.inputs], [model.gel_layer(configs['architecture']).get_layer(last_conv_layer_name).output, model.output]\n","    )\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = grad_model(img_array)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","\n","    # This is the gradient of the output neuron (top predicted or chosen)\n","    # with regard to the output feature map of the last conv layer\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the feature map array\n","    # by \"how important this channel is\" with regard to the top predicted class\n","    # then sum all the channels to obtain the heatmap class activation\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","    return heatmap.numpy()\n","\n","def create_gradcam(image, model, last_conv_layer_name, pred_index=None):\n","    # Preprocess the image array\n","    image, _ = preprocess(tf.expand_dims(image, axis=0), 0)\n","    # Get GradCAM\n","    heatmap = make_gradcam_heatmap(image, model, last_conv_layer_name, pred_index)\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # Use jet colormap to colorize heatmap\n","    jet = cm.get_cmap(\"jet\")\n","\n","    # Use RGB values of the colormap\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","    jet_heatmap = tf.image.resize(jet_heatmap, size=(28,28))\n","\n","    # Overlay\n","    superimposed_img = jet_heatmap * 0.4 + tf.squeeze(image, axis=0)\n","    superimposed_img = tf.clip_by_value(superimposed_img, 0.0, 1.0)\n","\n","    return superimposed_img"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"_yUye2ChI-w8"},"outputs":[],"source":["last_conv_layer_name = 'block4_conv3'"]},{"cell_type":"markdown","metadata":{"id":"m-cpGKdOI-w8"},"source":["In the cell block below, we will be using `WandbCallback`'s `validation_row_processor` and `prediction_row_processor` to log the images, ground truth label, model prediction and the GradCAM for model interpretability. \n","\n","The processors' take a callable function that receive an `ndx` (index) and a `row` (dict of data). The `validation_processor` function below receives the input image array along with target label as `row` dict. The `prediction_processor` receives  the model output prediction and the validation data row index. \n","\n","The `validation_row_processor` is executed when `WandbCallback` is initialized (i.e, before model training) while `prediction_row_processor` is called once the training is over. The `validation_row_processor` creates a table with two columns `input:image` and `target:class`. Notice that in the `prediction_processor` function we can get the logged image at a given `val_row` using the `get_row` method. "]},{"cell_type":"code","execution_count":42,"metadata":{"id":"25T7JVt4I-w8"},"outputs":[],"source":["def validation_processor(ndx, row):\n","    return {\n","        \"input:image\": wandb.Image(row[\"input\"]),\n","        \"target:class\": class_table.index_ref(row[\"target\"])\n","    }\n","\n","def prediction_processor(ndx, row):\n","    # Get the validation image\n","    valid_image = np.array(row[\"val_row\"].get_row()[\"input:image\"].image)\n","\n","    return {\n","        \"output:class\": class_table.index_ref(np.argmax(row[\"output\"])),\n","        \"gradcam\": wandb.Image(create_gradcam(valid_image, model, last_conv_layer_name)),\n","        \"output:logits\": {class_name: value for (class_name, value) in zip(list(config.class_names.values()), row[\"output\"].tolist())}\n","    }"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"PiJJ9HjCI-w8"},"outputs":[{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/bex_team/test_wandb/runs/1ohhud4w\" target=\"_blank\">comfy-planet-3</a></strong> to <a href=\"https://wandb.ai/bex_team/test_wandb\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","4/4 - 2s - loss: 1.5764 - accuracy: 0.4080 - val_loss: 1.5782 - val_accuracy: 0.5476 - 2s/epoch - 529ms/step\n","Epoch 2/5\n","4/4 - 1s - loss: 1.4525 - accuracy: 0.5040 - val_loss: 1.6258 - val_accuracy: 0.5238 - 691ms/epoch - 173ms/step\n","Epoch 3/5\n","4/4 - 1s - loss: 1.3123 - accuracy: 0.5680 - val_loss: 1.3180 - val_accuracy: 0.5714 - 757ms/epoch - 189ms/step\n","Epoch 4/5\n","4/4 - 1s - loss: 1.1044 - accuracy: 0.5360 - val_loss: 1.1636 - val_accuracy: 0.5476 - 762ms/epoch - 191ms/step\n","Epoch 5/5\n","4/4 - 1s - loss: 1.0386 - accuracy: 0.5040 - val_loss: 1.1101 - val_accuracy: 0.5238 - 782ms/epoch - 195ms/step\n","2/2 [==============================] - 0s 59ms/step - loss: 1.1101 - accuracy: 0.5238\n"]},{"data":{"text/html":["<br/>Waiting for W&B process to finish, PID 5752... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅█▇▅</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>evaluate/accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▆▅▂▁</td></tr><tr><td>val_accuracy</td><td>▄▁█▄▁</td></tr><tr><td>val_loss</td><td>▇█▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\">\n","<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.504</td></tr><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>1.11011</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>evaluate/accuracy</td><td>0.52381</td></tr><tr><td>loss</td><td>1.03862</td></tr><tr><td>val_accuracy</td><td>0.52381</td></tr><tr><td>val_loss</td><td>1.11011</td></tr></table>\n","</div></div>\n","Synced 6 W&B file(s), 0 media file(s), 45 artifact file(s) and 0 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">comfy-planet-3</strong>: <a href=\"https://wandb.ai/bex_team/test_wandb/runs/1ohhud4w\" target=\"_blank\">https://wandb.ai/bex_team/test_wandb/runs/1ohhud4w</a><br/>\n","Find logs at: <code>.\\wandb\\run-20220217_121339-1ohhud4w\\logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Initialize the W&B run\n","run = wandb.init(project=project_config['project_name'], config=configs, job_type='train')\n","config = wandb.config\n","\n","# Get validation table\n","data_art = run.use_artifact('bex_team/test_wandb/Brain_MRI_Images_for_Brain_Tumor_Detection:latest', type='dataset')\n","# data_art = run.use_artifact(f'{project_config[\"entity\"]}/{project_config[\"project_name\"]}/{configs[\"dataset\"]}:latest', type='dataset')\n","valid_table = data_art.get(\"valid_data\")\n","\n","# Create a class table\n","class_table = wandb.Table(columns=[], data=[])\n","class_table.add_column(\"class_name\", list(config.class_names.values()))\n","\n","# Define WandbCallback for experiment tracking\n","wandb_callback = WandbCallback(\n","                    log_evaluation=True,\n","                    validation_row_processor=lambda ndx, row: validation_processor(ndx, row),\n","                    prediction_row_processor=lambda ndx, row: prediction_processor(ndx, row),\n","                    validation_steps=4,\n","                    save_model=False\n","                )\n","\n","# callbacks\n","callbacks = [earlystopper, wandb_callback]#lr_callback\n","\n","# Train\n","model = train(config, callbacks=callbacks, verbose=2)\n","\n","# Evaluate the trained model\n","loss, acc = model.evaluate(np.array(X_val),np.array(y_val))\n","wandb.log({'evaluate/accuracy': acc})\n","\n","# Close the W&B run.\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"Copy of Keras_pipeline_with_Weights_and_Biases.ipynb","provenance":[{"file_id":"https://github.com/wandb/examples/blob/master/colabs/keras/Keras_pipeline_with_Weights_and_Biases.ipynb","timestamp":1645008414025}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}
