{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF:  2.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "# print(\"TF: \", tf.__version__)\n",
    "from keras import layers, models, Model\n",
    "import keras\n",
    "\n",
    "from pathlib import Path\n",
    "from keras.preprocessing.image import load_img, img_to_array, image_dataset_from_directory\n",
    "from keras.applications import vgg16, vgg19\n",
    "\n",
    "import utils\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.seed_everything()\n",
    "\n",
    "# Get base project directory\n",
    "project_path = Path(os.getcwd()).parent.parent\n",
    "datapath = (project_path /'data/processed/')\n",
    "\n",
    "CLASSES = {0 : 'yes', 1 : 'no'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset inspect\n",
    "# Read images to variables\n",
    "size = 224\n",
    "# X_train, y_train = get_training_set(CLASSES, size)\n",
    "X_train, y_train = utils.get_sets('train', CLASSES, size)\n",
    "X_val, y_val = utils.get_sets('val', CLASSES, size)\n",
    "X_test, y_test = utils.get_sets('test', CLASSES, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KHhaOEzCI-w0"
   },
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    image_width = X_train[0].shape[0],\n",
    "    image_height = X_train[0].shape[1],\n",
    "    image_channels = X_train[0].shape[2],\n",
    "    batch_size = 4,\n",
    "    class_names = CLASSES,\n",
    "    model_name = '', # set after model is defined\n",
    "    pretrain_weights = 'imagenet',\n",
    "    epochs = 5,\n",
    "    init_learning_rate = 0.001,\n",
    "    lr_decay_rate = 0.1,\n",
    "    optimizer = 'adam',\n",
    "    loss_fn = 'binary_crossentropy',\n",
    "    metrics = ['accuracy'],\n",
    "    earlystopping_patience = 5,\n",
    "    architecture = \"\",# To be defined f\"{base_model._name.upper()} global_average_pooling2d\",\n",
    "    dataset = \"Brain_MRI_Images_for_Brain_Tumor_Detection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolutional\n",
      "convolutional\n",
      "pooling\n",
      "convolutional\n",
      "convolutional\n",
      "pooling\n",
      "convolutional\n",
      "convolutional\n",
      "convolutional\n",
      "convolutional\n",
      "pooling\n",
      "convolutional\n",
      "convolutional\n",
      "convolutional\n",
      "convolutional\n",
      "pooling\n",
      "convolutional\n",
      "convolutional\n",
      "convolutional\n",
      "convolutional\n",
      "pooling\n",
      "['input_1', 'block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_conv4', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_conv4', 'block4_pool', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4', 'block5_pool']\n"
     ]
    }
   ],
   "source": [
    "list_names = []\n",
    "shape = (X_train[0].shape[0], X_train[0].shape[1], 3)\n",
    "base_model = vgg19.VGG19(weights=configs['pretrain_weights'], include_top=False, input_shape=shape)\n",
    "base_model.trainable = False\n",
    "for layer in base_model.layers:\n",
    "    if str(type(layer)).split('.')[2] != 'input_layer':\n",
    "        print(str(type(layer)).split('.')[2])\n",
    "    list_names.append(layer._name)\n",
    "print(list_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.input_layer.InputLayer at 0x25f0b1ad730>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wmjJlRYqI-w6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Baseline_VGG19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,897\n",
      "Trainable params: 513\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(input_shape: tuple=(32, 32, 3),\n",
    "              output_activation: str='sigmoid'):\n",
    "    shape = (X_train[0].shape[0], X_train[0].shape[1], 3)\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    base_model = vgg19.VGG19(weights=configs['pretrain_weights'], include_top=False, input_shape=shape)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    configs['architecture'] = base_model._name\n",
    "\n",
    "    # inputs = layers.Input(shape=list(shape))\n",
    "    # inputs = base_model.layers[0].input\n",
    "    # x = vgg19.preprocess_input(inputs)\n",
    "    # for c, layer in enumerate(base_model.layers):\n",
    "    #     if c >= 0:\n",
    "    #         x = layer(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    \n",
    "    outputs = layers.Dense(1, activation=output_activation)(x)\n",
    "    model  = models.Model(base_model.input, outputs, name=f'Baseline_{base_model._name.upper()}')\n",
    "    return model\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_filepath = (Path(os.getcwd()) /'model_checkpoint/model_checkpoint')\n",
    "\n",
    "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_filepath,\n",
    "#     save_weights_only=True,\n",
    "#     monitor='val_accuracy',\n",
    "#     mode='max',\n",
    "#     save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XUld9o-zI-w6"
   },
   "outputs": [],
   "source": [
    "earlystopper = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=configs['earlystopping_patience'], verbose=0, mode='auto',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kORq8cZJI-w7"
   },
   "outputs": [],
   "source": [
    "def train(config: dict,\n",
    "          callbacks: list,\n",
    "          verbose: int=0):\n",
    "    \"\"\"\n",
    "    Utility function to train the model.\n",
    "\n",
    "    Arguments:\n",
    "        config (dict): Dictionary of hyperparameters.\n",
    "        callbacks (list): List of callbacks passed to `model.fit`.\n",
    "        verbose (int): 0 for silent and 1 for progress bar.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initalize model\n",
    "    # tf.keras.backend.clear_session()\n",
    "    model = get_model(input_shape=(config['image_width'], config['image_height'], config['image_channels']))\n",
    "    config['model_name'] = model.name # set\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config['init_learning_rate'])\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=config['loss_fn'],\n",
    "                  metrics=config['metrics'])\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=metrics)\n",
    "    # Train the model\n",
    "    _ = model.fit(vgg19.preprocess_input(X_train), y_train,\n",
    "                  epochs=config['epochs'],\n",
    "                  validation_data=(np.array(X_val),np.array(y_val)),\n",
    "                  callbacks=callbacks,\n",
    "                  batch_size=config['batch_size'],\n",
    "                  verbose=verbose)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dbrdw5KoI-w7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 - 3s - loss: 1.2489 - accuracy: 0.5280 - val_loss: 1.5742 - val_accuracy: 0.5238 - 3s/epoch - 80ms/step\n",
      "Epoch 2/5\n",
      "32/32 - 1s - loss: 0.7659 - accuracy: 0.6800 - val_loss: 1.5392 - val_accuracy: 0.5952 - 1s/epoch - 42ms/step\n",
      "Epoch 3/5\n",
      "32/32 - 1s - loss: 0.6041 - accuracy: 0.7280 - val_loss: 1.3192 - val_accuracy: 0.5952 - 1s/epoch - 42ms/step\n",
      "Epoch 4/5\n",
      "32/32 - 1s - loss: 0.4880 - accuracy: 0.7840 - val_loss: 1.2054 - val_accuracy: 0.6190 - 1s/epoch - 42ms/step\n",
      "Epoch 5/5\n",
      "32/32 - 1s - loss: 0.4199 - accuracy: 0.8080 - val_loss: 1.2300 - val_accuracy: 0.6429 - 1s/epoch - 40ms/step\n"
     ]
    }
   ],
   "source": [
    "config = configs\n",
    "callbacks = [earlystopper]\n",
    "\n",
    "model = train(config, callbacks=callbacks, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_yUye2ChI-w8"
   },
   "outputs": [],
   "source": [
    "last_conv_layer_name = 'block5_conv4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 299x299\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        #[model.inputs], [model.get_layer('vgg19').get_layer(last_conv_layer_name).output, model.output]\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    \n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for l in model.get_layer('vgg19').layers:\n",
    "#     print(l._name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.expand_dims(X_train[0], axis=0)\n",
    "# x = keras.preprocessing.image.img_to_array(x)\n",
    "# Print what the top predicted class is\n",
    "preds = model.predict(x)\n",
    "\n",
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(x, model, \"block5_conv4\")\n",
    "\n",
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers:\n",
    "    print(l._name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67f495f3a98c863f9b203ff8f335ccecec58403956d94b2cbc6c3bef6b2dc434"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('ecpython')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
