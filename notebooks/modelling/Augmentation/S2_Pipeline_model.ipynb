{"cells":[{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":972},"executionInfo":{"elapsed":107924,"status":"ok","timestamp":1645806720231,"user":{"displayName":"Jimmy Nilsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604156050067165706"},"user_tz":-60},"id":"fEjD-RrDEax6","outputId":"00e43287-6d9b-4448-9834-d02e8eedd21a"},"outputs":[],"source":["# Setup if running in colab\n","RunningInCOLAB = 'google.colab' in str(get_ipython())\n","if RunningInCOLAB:\n","  try:\n","    if runonce:\n","      print(\"Already ran\")\n","  \n","  except:\n","    runonce = True\n","    !pip install wandb\n","    !git clone https://github.com/Jimmy-Nnilsson/StudieGrupp3_MLProjekt.git\n","    \n","    import wandb\n","    wandb.login()"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1645806720232,"user":{"displayName":"Jimmy Nilsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604156050067165706"},"user_tz":-60},"id":"R643QH_ogQ2i","outputId":"716ebd20-0120-4620-888e-335ab833f912"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"data":{"text/plain":["True"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3848,"status":"ok","timestamp":1645806724067,"user":{"displayName":"Jimmy Nilsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604156050067165706"},"user_tz":-60},"id":"gr84aH0mEYh8","outputId":"34b3e98c-dce9-4a79-e48c-45b5d19e6ba6"},"outputs":[{"name":"stdout","output_type":"stream","text":["TF:  2.7.0\n"]}],"source":["import os\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import tensorflow as tf\n","print(\"TF: \", tf.__version__)\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from keras import Model\n","import keras\n","\n","from pathlib import Path\n","from keras.preprocessing.image import load_img, img_to_array, image_dataset_from_directory\n","from tensorflow.keras.applications import vgg16, vgg19, mobilenet_v2, inception_v3\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","import wandb\n","from wandb.keras import WandbCallback\n","\n","import cv2"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1645806724067,"user":{"displayName":"Jimmy Nilsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604156050067165706"},"user_tz":-60},"id":"tc9140-sEYiB"},"outputs":[],"source":["def seed_everything():\n","    # os.environ['PYTHONHASHSEED'] = '0'\n","    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n","    random.seed(1254)\n","    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n","    tf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n","\n","seed_everything()"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1645806724068,"user":{"displayName":"Jimmy Nilsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604156050067165706"},"user_tz":-60},"id":"Cnn3A9AdEYiC"},"outputs":[],"source":["# Get base project directory\n","if not RunningInCOLAB:\n","#   project_path = Path(os.getcwd()).parent.parent\n","\n","  for i, p in enumerate(Path(os.getcwd()).parts):\n","    if p == \"StudieGrupp3_MLProjekt\":\n","        break\n","    pathparts = list(Path(os.getcwd()).parts[0:i+2])\n","    project_path = Path(pathparts[0],\"\\\\\\\\\".join(pathparts[1:]))\n","else:\n","  project_path = Path('/content/StudieGrupp3_MLProjekt/')\n","datapath = (project_path /'data/processed/')\n","\n","CLASSES = {0 : 'yes', 1 : 'no'}\n","# Loops through pathlist and reads and resizes images\n","def read_image(pathlist : list, size : int)-> list:\n","    data = []\n","    for path in pathlist:\n","        image=load_img(path, color_mode='rgb', target_size=(size, size))\n","        # image=load_img(path, color_mode='rgb', target_size=(size, size))\n","        image=img_to_array(image)\n","        # image=image/255.0\n","        data.append(image)\n","    data = np.asarray(data, dtype=np.uint8)\n","    return data\n","\n","# Makes input and label data from folder locations.\n","# Loops through location \"subfolder/CLASSES\"\n","def get_sets(subfolder : str, CLASSES : dict, size : int):\n","    folder_paths = []\n","    folder_labels = []\n","    labels = []\n","    for k,v in CLASSES.items():\n","        # input datapath generation\n","        folder_paths += list((datapath / f\"2_split_{v}/{subfolder}\").rglob(\"*\"))\n","    # Label data generation\n","    folder_labels = [0 if x.stem.split('_')[1] == 'yes' else 1 for x in folder_paths]\n","    folder_labels = np.asarray(folder_labels, dtype=np.uint8)\n","    # Extract images from datapaths\n","    img_list = read_image(folder_paths, size)\n","\n","    return img_list, folder_labels\n","\n","def get_training_set(CLASSES : dict, size : int):\n","    folder_paths = []\n","    folder_labels = []\n","    labels = []\n","    for k,v in CLASSES.items():\n","        # input datapath generation\n","        # folder_paths += list((datapath / f\"3_aug_{v}_train/\").rglob(\"*\"))\n","        # folder_paths += list((datapath / f\"3_augmentation_train/3_aug_geo_{v}_train/\").rglob(\"*\"))\n","        folder_paths += list((datapath / f\"3_augmentation_train/3_aug_pix_{v}_train/\").rglob(\"*\"))\n","        # print(folder_paths)\n","    # Label data generation\n","    folder_labels = [0 if x.stem.split('_')[1] == 'yes' else 1 for x in folder_paths]\n","    # Extract images from datapaths\n","    img_list = read_image(folder_paths, size)\n","\n","    return img_list, folder_labels"]},{"cell_type":"markdown","metadata":{"id":"1jDJ9plmgxlC"},"source":["Load Pictures"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":35987,"status":"ok","timestamp":1645806760050,"user":{"displayName":"Jimmy Nilsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604156050067165706"},"user_tz":-60},"id":"Fc4RQseIEYiE"},"outputs":[],"source":["# Dataset inspect\n","# Read images to variables\n","size = 224\n","X_aug_train, y_aug_train = get_training_set(CLASSES, size)\n","X_train, y_train = get_sets('train', CLASSES, size)\n","X_val, y_val = get_sets('val', CLASSES, size)\n","X_test, y_test = get_sets('test', CLASSES, size)"]},{"cell_type":"markdown","metadata":{"id":"S2vj4b_rgxlF"},"source":["Data loader"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"BACMLdC0I-w3"},"outputs":[],"source":["#@title\n","@tf.function\n","def preprocess(image: tf.Tensor, label: tf.Tensor):\n","    \"\"\"\n","    Preprocess the image tensors and parse the labels\n","    \"\"\"\n","    # Preprocess images\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","\n","    # Parse label\n","    label = tf.cast(label, tf.float32)\n","\n","    return image, label\n","\n","\n","def prepare_dataloader(images: np.ndarray,\n","                       labels: np.ndarray,\n","                       loader_type: str='train',\n","                       batch_size: int=128):\n","    \"\"\"\n","    Utility function to prepare dataloader.\n","    \"\"\"\n","    images = model_preprocess(images)\n","    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n","\n","    if loader_type=='train':\n","        dataset = dataset.shuffle(1024)\n","\n","    dataloader = (\n","        dataset\n","        .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n","        .batch(batch_size)\n","        .prefetch(tf.data.AUTOTUNE)\n","    )\n","\n","    return dataloader\n","\n","def model_preprocess(images):\n","    images = vgg19.preprocess_input(images)\n","    # images = vgg16.preprocess_input(images)\n","    # images = mobilenet_v2.preprocess_input(images)\n","    # images = inception_v3.preprocess_input(images)\n","    \n","    return images"]},{"cell_type":"markdown","metadata":{"id":"dAr1WwrYgxlH"},"source":["Config parameters"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"KHhaOEzCI-w0"},"outputs":[],"source":["# Mind model processing\n","# Finetune not complete\n","configs = dict(\n","    project_name = \"Augmentation_pipe\", #Project Name\n","    mode = 'disabled', #{'offline', 'run', 'disabled', 'dryrun', 'online'} # WandB run status\n","    job_type = \"\", #Run type for WandB\n","    group = \"\", # Group in WandB\n","    sub_group = \"_pipeline\",\n","\n","    class_names = CLASSES, # Classes for training\n","    training_set = \"\",\n","\n","    image_width = X_train[0].shape[0], # Picture width for model input\n","    image_height = X_train[0].shape[1], # Picture height for model input\n","    image_channels = X_train[0].shape[2], # Picture channels for model input\n","\n","    pretrain_weights = 'imagenet', # pretrained weights for basemodel if any\n","    batch_size = 16, # Batchsize for training\n","    init_learning_rate = 0.000024, # Initial training rate if no callback is used\n","    lr_decay_rate = 0.1, #decayrate of training rate\n","    epochs = 100, # Epochs to train\n","    optimizer = 'rmsprop', # The optimizer used by the ml model\n","    loss_fn = 'binary_crossentropy', # Loss function\n","    metrics = ['accuracy'], # Metrics\n","    earlystopping_patience = 5, # For the early stopping callback\n","\n","    dataset = \"Brain_MRI_Images_for_Brain_Tumor_Detection\",\n","    fine_tune_learning_rate = 1e-5, # learningrate Used during fine tuning\n","    fine_tune_epochs = 10, # Epochs ran at finetuning\n","\n","    architecture = \"\",# To be defined f\"{base_model._name.upper()} global_average_pooling2d\",\n","    model_name = '' # set after model is defined # Name of the ml Model\n","\n",")"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"dVtM4jajI-w4"},"outputs":[],"source":["trainloader = prepare_dataloader(X_train, y_train, 'train', configs.get('batch_size', 64))\n","augtrainloader = prepare_dataloader(X_aug_train, y_aug_train, 'train', configs.get('batch_size', 64))\n","validloader = prepare_dataloader(X_val, y_val, 'valid', configs.get('batch_size', 64))\n","# testloader = prepare_dataloader(X_test, y_test, 'test', configs.get('batch_size', 64))"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"82sEYhox889L"},"outputs":[],"source":["configs['training_set'] = augtrainloader"]},{"cell_type":"markdown","metadata":{"id":"dpjWJwT9gxlJ"},"source":["Model class definition"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"clfaakDVgxlJ"},"outputs":[],"source":["class Model_Class():\n","    def __init__(self,\n","                 output_activation: str='sigmoid'\n","                 ):\n","\n","        self.shape = (configs['image_width'],configs['image_height'], configs['image_channels'])\n","        self.model = \"\"\n","        # self.base_model = \"\"\n","        self.output_activation = output_activation\n","        self.run = \"\"\n","        self.get_model()\n","        self.conv_layers, self.layer_names = self.__get_convlayers()\n","        self.preds = \"\"\n","        if not wandb.run is None: wandb.finish()\n","\n","    def get_model(self):\n","        tf.keras.backend.clear_session()\n","        kwarg = dict(weights=configs['pretrain_weights'], include_top=False, input_shape=self.shape)\n","        self.base_model = vgg19.VGG19(**kwarg)\n","        # self.base_model = vgg16.VGG16(**kwarg)\n","        # self.base_model = mobilenet_v2.MobileNetV2(**kwarg)\n","        # self.base_model = inception_v3.InceptionV3(**kwarg)\n","        \n","        self.base_model.trainable = False\n","\n","        x = layers.GlobalAveragePooling2D()(self.base_model.output)\n","        # x = layers.Flatten()(self.base_model.output)\n","\n","        # x = layers.Dense(128, activation='relu')(x)\n","        # x = layers.Dense(64, activation='relu')(x)\n","        # x = layers.Dense(32, activation='relu')(x)\n","        outputs = layers.Dense(1, activation=self.output_activation)(x)\n","        configs['group'] = f'{self.base_model._name}{configs[\"sub_group\"]}'\n","        configs['architecture'] = self.base_model._name\n","        self.model  = models.Model(self.base_model.input, outputs, name=f'Baseline_{self.base_model._name.upper()}')\n","\n","    def load_model(self, path):\n","        self.model = tf.keras.models.load_model(path)\n","\n","    def transfer_learning(self,\n","        callbacks: list,\n","        verbose: int=0,\n","        wb: bool=False): # Makes training run on all but base model\n","\n","        train_config = {\n","            \"learning_rate\" : configs['init_learning_rate'],\n","            \"epochs\" : configs['epochs'],\n","            \"compile\" : True}\n","        configs['job_type'] = \"Transfer learning\"\n","\n","        self.base_model.trainable = False\n","        self.__train(callbacks=callbacks, verbose=verbose, wb=wb, train_config=train_config)\n","\n","    def fine_tune(self,\n","        callbacks: list,\n","        verbose: int=0,\n","        wb: bool=False,\n","        trainable_layers: list=0):\n","\n","        train_config = {\n","            \"learning_rate\" : configs['fine_tune_learning_rate'],\n","            \"epochs\" : configs['fine_tune_epochs'],\n","            'job_type' : 'Fine-tune',\n","            \"compile\" : False}\n","        configs['job_type'] = \"Fine-tune\"\n","        self.set_trainable(trainable_layers)\n","        self.__train(callbacks=callbacks, verbose=verbose, wb=wb, train_config=train_config)\n","\n","    def __train(self,\n","        callbacks: list,\n","        verbose: int=0,\n","        wb: bool=False,\n","        train_config:dict={}\n","        ):\n","\n","        if wb:\n","            callbacks.append(self.__wandb())\n","\n","        # Initalize model\n","        # tf.keras.backend.clear_session()\n","        configs['model_name'] = self.model._name # set\n","\n","        # Compile the model\n","        \n","        opt = tf.keras.optimizers.SGD(learning_rate=train_config['learning_rate'])\n","        #   opt = tf.keras.optimizers.RMSprop(learning_rate=train_config['learning_rate'])\n","        self.model.compile(optimizer=opt,\n","                    loss=configs['loss_fn'],\n","                    metrics=configs['metrics'])\n","        \n","        # Train model\n","        _ = self.model.fit(configs['training_set'],\n","                    epochs=train_config['epochs'],\n","                    validation_data=validloader,\n","                    callbacks=callbacks,\n","                    verbose=verbose)\n","        if wb:\n","            # Evaluate the trained model\n","            loss, acc = self.model.evaluate(validloader)\n","            self.run.log({'evaluate/accuracy': acc})\n","\n","\n","            # Close the W&B run.\n","            self.run.finish()\n","\n","    def grad_cam(self, image, layer=None):\n","        self.preds = self.predict(image)\n","\n","        if type(layer) is list:\n","            heatmap_list, superimposed_list = {},{}\n","            for layer_num in layer:\n","                heatmap = self.make_gradcam_heatmap(np.expand_dims(image, axis=0), layer_num, np.argmax(self.preds[0]))\n","                superimposed_img = self.superimpose(image,heatmap)\n","                heatmap_list[self.model.layers[layer_num]._name] = heatmap\n","                superimposed_list[self.model.layers[layer_num]._name] = superimposed_img\n","            return image, heatmap_list, superimposed_list\n","        else:\n","            heatmap = self.make_gradcam_heatmap(np.expand_dims(image, axis=0), layer, np.argmax(self.preds[0]))\n","            superimposed_img = self.superimpose(image,heatmap)\n","            return image, heatmap, superimposed_img\n","\n","\n","    def make_gradcam_heatmap(self, img_array, layer=None, pred_index=None):\n","        # First, we create a model that maps the input image to the activations\n","        # of the last conv layer as well as the output predictions\n","        if layer == None: layer=self.conv_layers[-1]\n","        model = self.model\n","        grad_model = Model(\n","            [model.inputs], [model.layers[layer].output, model.output]\n","            # [model.inputs], [model.get_layer(self.layer_names[-1]).output, model.output]\n","        )\n","\n","        # Then, we compute the gradient of the top predicted class for our input image\n","        # with respect to the activations of the last conv layer\n","        with tf.GradientTape() as tape:\n","            last_conv_layer_output, preds = grad_model(img_array)\n","            if pred_index is None:\n","                pred_index = tf.argmax(preds[0])\n","            class_channel = preds[:, pred_index]\n","\n","        # This is the gradient of the output neuron (top predicted or chosen)\n","        # with regard to the output feature map of the last conv layer\n","        grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","        # This is a vector where each entry is the mean intensity of the gradient\n","        # over a specific feature map channel\n","        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","        # We multiply each channel in the feature map array\n","        # by \"how important this channel is\" with regard to the top predicted class\n","        # then sum all the channels to obtain the heatmap class activation\n","        last_conv_layer_output = last_conv_layer_output[0]\n","        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","        heatmap = tf.squeeze(heatmap)\n","\n","        # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","        return heatmap.numpy()\n","\n","    def predict(self, pic):\n","        x = model_preprocess(pic)\n","        if len(x.shape) < 4:\n","          x = np.expand_dims(x, axis=0)\n","        \n","        preds = self.model.predict(x)\n","        return preds\n","\n","    def superimpose(self, pic,heatmap):\n","        img_numpy = np.asarray(np.clip(pic, 0, 190))\n","\n","        heatmap_resized = cv2.resize(heatmap, (img_numpy.shape[1], img_numpy.shape[0]))\n","        heatmap_resized = np.uint8(255 * heatmap_resized)\n","        heatmap_resized = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)\n","\n","        superimposed_img = 0.3*heatmap_resized[:,:,::-1] + img_numpy\n","        superimposed_img = superimposed_img.astype(np.uint8)\n","        return superimposed_img\n","\n","    def __get_convlayers(self):\n","        list_conv_layers = []\n","        list_layer_names = []\n","        for i,l in enumerate(self.model.layers):\n","            # print(str(l).split('.'))\n","            if str(l).split('.')[2] == 'convolutional':\n","                list_conv_layers.append(i)\n","                list_layer_names.append(l._name)\n","        return list_conv_layers, list_layer_names\n","\n","    def __wandb(self): \n","        self.run = wandb.init(mode=configs['mode'], entity='bex_team' ,project=configs['project_name'], config=configs, job_type=configs['job_type'], group=configs['group'],)\n","\n","        # Define WandbCallback for experiment tracking\n","        wandb_callback = WandbCallback(monitor='val_loss',\n","                                    log_weights=True,\n","                                    log_evaluation=True,\n","                                    validation_steps=5,\n","                                    save_model=True,\n","                                    save_graph = True\n","                                    )\n","        return wandb_callback\n","\n","    def set_trainable(self, trainable_layers=0): # Sets whole model to trainable\n","        if trainable_layers == 0:\n","            self.model.trainable = True\n","        else:\n","            self.model.trainable = False\n","            for i in trainable_layers:\n","                self.model.layers[i].trainable = True"]},{"cell_type":"markdown","metadata":{"id":"N7Z16NM-gxlL"},"source":["Call model"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"M9A5yCANgxlN"},"outputs":[],"source":["model = Model_Class()"]},{"cell_type":"markdown","metadata":{},"source":["Change to preset model. (95%)"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["model_path = (project_path / \"models/first_golden_model.h5\")\n","model.load_model(model_path)"]},{"cell_type":"markdown","metadata":{"id":"6Tg4OnwwgxlI"},"source":["Model Callbacks"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ir6gxKIOEYiM"},"outputs":[],"source":["model_filename = f'{model.base_model._name}_{configs[\"project_name\"]}{configs[\"sub_group\"]}.h5'\n","if not RunningInCOLAB:\n","  checkpoint_filepath = (Path(os.getcwd()) /f'{model_filename}')\n","else:\n","  checkpoint_filepath = (Path(f'/content/{model_filename}'))\n","\n","model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)\n"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"XUld9o-zI-w6"},"outputs":[],"source":["earlystopper = keras.callbacks.EarlyStopping(\n","    monitor='val_loss', patience=configs['earlystopping_patience'], verbose=0, mode='auto',\n","    restore_best_weights=True\n",")"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"tIzmu5iFgxlJ"},"outputs":[],"source":["def lr_scheduler(epoch, lr):\n","    # log the current learning rate onto W&B\n","\n","    if epoch < 12:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-configs['lr_decay_rate'])\n","\n","lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"]},{"cell_type":"markdown","metadata":{"id":"7BWGilQqgxlN"},"source":["Make gradcams"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"2usvE1YxwzhN"},"outputs":[],"source":["def plot_gradcam(image, heatmap, superimposed_img):\n","  if type(heatmap) == dict and type(superimposed_img) == dict:\n","      nlen = len(heatmap)\n","      fig, ax = plt.subplots(nlen,3, figsize=(10, nlen*3.5))\n","      # fig.figsize=(20,20)\n","      for i, (k, img) in enumerate(heatmap.items()):\n","          ax[i,0].imshow(img)\n","          ax[i,1].set_title(k)\n","          ax[i,1].imshow(superimposed_img[k])\n","          ax[i,2].imshow(image)\n","  else:\n","    plt.subplot(1,3, 1)\n","    plt.imshow(heatmap)\n","    plt.subplot(1,3, 2)\n","    plt.imshow(superimposed_img)\n","    plt.subplot(1,3, 3)\n","    plt.imshow(image)"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":10082,"status":"ok","timestamp":1645782836880,"user":{"displayName":"Jimmy Nilsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604156050067165706"},"user_tz":-60},"id":"6-H_exnJgxlN","outputId":"1eb3b4e7-ab1d-4722-cd49-597b0bdc10b7"},"outputs":[],"source":["# image, heatmap, superimposed_img = model.grad_cam(X_train[0], model.conv_layers)\n","# plot_gradcam(image, heatmap, superimposed_img)"]},{"cell_type":"markdown","metadata":{"id":"QInYauoxgxlO"},"source":["Plot gradcams"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1645782837448,"user":{"displayName":"Jimmy Nilsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604156050067165706"},"user_tz":-60},"id":"AxzuBr6qgxlO","outputId":"efeffee2-25e6-41c9-fe78-120045df5cc3"},"outputs":[],"source":["# image, heatmap, superimposed_img = model.grad_cam(X_train[0])\n","# plot_gradcam(image, heatmap, superimposed_img)"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"E8oDvtHG5fH5"},"outputs":[],"source":["import seaborn as sns\n","\n","def crcm(model, x, y, p=True):\n","    y_pred = model.predict(x)\n","    y_pred = np.asarray(y_pred)\n","    y_pred = np.uint8(y_pred+0.5)\n","    cm = confusion_matrix(y, y_pred)\n","    cr = classification_report(y, y_pred)\n","    if p:\n","        plt.xlabel('Pred')\n","        sns.heatmap(cm, vmin=0, annot=True)\n","        print(cr)\n","    cr = classification_report(y, y_pred, output_dict=True )\n","    return cm, cr"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1645782837450,"user":{"displayName":"Jimmy Nilsson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16604156050067165706"},"user_tz":-60},"id":"JV8x_bUdJJ5L","outputId":"40d38681-609b-4762-beea-ce7b26c58587"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.85      0.97      0.91        30\n","           1       0.88      0.58      0.70        12\n","\n","    accuracy                           0.86        42\n","   macro avg       0.86      0.78      0.80        42\n","weighted avg       0.86      0.86      0.85        42\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["cm_, cr_ = crcm(model, X_val, y_val)"]},{"cell_type":"markdown","metadata":{"id":"enLx-uT8gxlO"},"source":["Train with wandb"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjgcyPZvgxlO","outputId":"4cc808d8-0a61-4b41-e246-c17d45b3837e"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Error initializing ValidationDataLogger in WandbCallback. Skipping logging validation data. Error: \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","157/157 - 14s - loss: 0.5517 - accuracy: 0.8724 - val_loss: 0.2025 - val_accuracy: 0.9286 - 14s/epoch - 88ms/step\n","Epoch 2/100\n","157/157 - 13s - loss: 0.5046 - accuracy: 0.8820 - val_loss: 0.2107 - val_accuracy: 0.8810 - 13s/epoch - 80ms/step\n","Epoch 3/100\n","157/157 - 12s - loss: 0.4871 - accuracy: 0.8816 - val_loss: 0.2214 - val_accuracy: 0.8810 - 12s/epoch - 78ms/step\n","Epoch 4/100\n","157/157 - 12s - loss: 0.4759 - accuracy: 0.8816 - val_loss: 0.2272 - val_accuracy: 0.8810 - 12s/epoch - 78ms/step\n","Epoch 5/100\n","157/157 - 12s - loss: 0.4659 - accuracy: 0.8796 - val_loss: 0.2309 - val_accuracy: 0.8810 - 12s/epoch - 79ms/step\n","Epoch 6/100\n","157/157 - 13s - loss: 0.4568 - accuracy: 0.8820 - val_loss: 0.2327 - val_accuracy: 0.8810 - 13s/epoch - 80ms/step\n","Epoch 7/100\n","157/157 - 13s - loss: 0.4482 - accuracy: 0.8828 - val_loss: 0.2319 - val_accuracy: 0.8810 - 13s/epoch - 80ms/step\n","Epoch 8/100\n","157/157 - 12s - loss: 0.4383 - accuracy: 0.8828 - val_loss: 0.2320 - val_accuracy: 0.8810 - 12s/epoch - 78ms/step\n","Epoch 9/100\n","157/157 - 12s - loss: 0.4295 - accuracy: 0.8844 - val_loss: 0.2329 - val_accuracy: 0.8810 - 12s/epoch - 75ms/step\n","Epoch 10/100\n","157/157 - 12s - loss: 0.4210 - accuracy: 0.8864 - val_loss: 0.2343 - val_accuracy: 0.8571 - 12s/epoch - 75ms/step\n","Epoch 11/100\n","157/157 - 12s - loss: 0.4134 - accuracy: 0.8852 - val_loss: 0.2352 - val_accuracy: 0.8571 - 12s/epoch - 75ms/step\n","Epoch 12/100\n","157/157 - 12s - loss: 0.4046 - accuracy: 0.8872 - val_loss: 0.2364 - val_accuracy: 0.8810 - 12s/epoch - 78ms/step\n","Epoch 13/100\n","157/157 - 12s - loss: 0.3974 - accuracy: 0.8872 - val_loss: 0.2364 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 14/100\n","157/157 - 12s - loss: 0.3906 - accuracy: 0.8908 - val_loss: 0.2362 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 15/100\n","157/157 - 12s - loss: 0.3835 - accuracy: 0.8904 - val_loss: 0.2377 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 16/100\n","157/157 - 12s - loss: 0.3772 - accuracy: 0.8908 - val_loss: 0.2392 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 17/100\n","157/157 - 12s - loss: 0.3719 - accuracy: 0.8896 - val_loss: 0.2398 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 18/100\n","157/157 - 12s - loss: 0.3662 - accuracy: 0.8928 - val_loss: 0.2405 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 19/100\n","157/157 - 12s - loss: 0.3605 - accuracy: 0.8948 - val_loss: 0.2413 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 20/100\n","157/157 - 12s - loss: 0.3550 - accuracy: 0.8932 - val_loss: 0.2432 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 21/100\n","157/157 - 12s - loss: 0.3500 - accuracy: 0.8944 - val_loss: 0.2439 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 22/100\n","157/157 - 12s - loss: 0.3452 - accuracy: 0.8932 - val_loss: 0.2437 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 23/100\n","157/157 - 12s - loss: 0.3399 - accuracy: 0.8924 - val_loss: 0.2447 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 24/100\n","157/157 - 12s - loss: 0.3355 - accuracy: 0.8952 - val_loss: 0.2451 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 25/100\n","157/157 - 12s - loss: 0.3315 - accuracy: 0.8952 - val_loss: 0.2454 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 26/100\n","157/157 - 12s - loss: 0.3270 - accuracy: 0.8952 - val_loss: 0.2464 - val_accuracy: 0.8810 - 12s/epoch - 75ms/step\n","Epoch 27/100\n","157/157 - 12s - loss: 0.3235 - accuracy: 0.8976 - val_loss: 0.2468 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 28/100\n","157/157 - 12s - loss: 0.3198 - accuracy: 0.8960 - val_loss: 0.2475 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 29/100\n","157/157 - 12s - loss: 0.3163 - accuracy: 0.8960 - val_loss: 0.2485 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 30/100\n","157/157 - 12s - loss: 0.3133 - accuracy: 0.8964 - val_loss: 0.2478 - val_accuracy: 0.8810 - 12s/epoch - 78ms/step\n","Epoch 31/100\n","157/157 - 12s - loss: 0.3095 - accuracy: 0.8984 - val_loss: 0.2489 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 32/100\n","157/157 - 12s - loss: 0.3063 - accuracy: 0.8980 - val_loss: 0.2498 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 33/100\n","157/157 - 12s - loss: 0.3033 - accuracy: 0.8956 - val_loss: 0.2498 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 34/100\n","157/157 - 12s - loss: 0.3003 - accuracy: 0.8968 - val_loss: 0.2500 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 35/100\n","157/157 - 12s - loss: 0.2972 - accuracy: 0.8988 - val_loss: 0.2509 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 36/100\n","157/157 - 12s - loss: 0.2950 - accuracy: 0.8984 - val_loss: 0.2511 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 37/100\n","157/157 - 12s - loss: 0.2925 - accuracy: 0.8976 - val_loss: 0.2510 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 38/100\n","157/157 - 12s - loss: 0.2898 - accuracy: 0.8996 - val_loss: 0.2513 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 39/100\n","157/157 - 12s - loss: 0.2872 - accuracy: 0.9016 - val_loss: 0.2525 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 40/100\n","157/157 - 12s - loss: 0.2856 - accuracy: 0.8992 - val_loss: 0.2527 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 41/100\n","157/157 - 12s - loss: 0.2831 - accuracy: 0.9004 - val_loss: 0.2531 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 42/100\n","157/157 - 12s - loss: 0.2813 - accuracy: 0.9000 - val_loss: 0.2536 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 43/100\n","157/157 - 12s - loss: 0.2793 - accuracy: 0.9016 - val_loss: 0.2540 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 44/100\n","157/157 - 12s - loss: 0.2771 - accuracy: 0.9020 - val_loss: 0.2557 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 45/100\n","157/157 - 12s - loss: 0.2755 - accuracy: 0.9036 - val_loss: 0.2564 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 46/100\n","157/157 - 12s - loss: 0.2738 - accuracy: 0.9040 - val_loss: 0.2577 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 47/100\n","157/157 - 12s - loss: 0.2726 - accuracy: 0.9032 - val_loss: 0.2567 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 48/100\n","157/157 - 12s - loss: 0.2704 - accuracy: 0.9020 - val_loss: 0.2575 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 49/100\n","157/157 - 12s - loss: 0.2691 - accuracy: 0.9028 - val_loss: 0.2578 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 50/100\n","157/157 - 12s - loss: 0.2674 - accuracy: 0.9032 - val_loss: 0.2586 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 51/100\n","157/157 - 12s - loss: 0.2661 - accuracy: 0.9036 - val_loss: 0.2588 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 52/100\n","157/157 - 12s - loss: 0.2647 - accuracy: 0.9036 - val_loss: 0.2591 - val_accuracy: 0.8810 - 12s/epoch - 78ms/step\n","Epoch 53/100\n","157/157 - 12s - loss: 0.2631 - accuracy: 0.9048 - val_loss: 0.2590 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 54/100\n","157/157 - 12s - loss: 0.2615 - accuracy: 0.9028 - val_loss: 0.2601 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 55/100\n","157/157 - 12s - loss: 0.2605 - accuracy: 0.9068 - val_loss: 0.2603 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 56/100\n","157/157 - 12s - loss: 0.2590 - accuracy: 0.9044 - val_loss: 0.2610 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 57/100\n","157/157 - 12s - loss: 0.2579 - accuracy: 0.9048 - val_loss: 0.2612 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 58/100\n","157/157 - 12s - loss: 0.2567 - accuracy: 0.9056 - val_loss: 0.2611 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 59/100\n","157/157 - 12s - loss: 0.2553 - accuracy: 0.9064 - val_loss: 0.2617 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 60/100\n","157/157 - 12s - loss: 0.2543 - accuracy: 0.9056 - val_loss: 0.2621 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 61/100\n","157/157 - 12s - loss: 0.2531 - accuracy: 0.9068 - val_loss: 0.2624 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 62/100\n","157/157 - 12s - loss: 0.2522 - accuracy: 0.9052 - val_loss: 0.2628 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 63/100\n","157/157 - 12s - loss: 0.2511 - accuracy: 0.9068 - val_loss: 0.2627 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 64/100\n","157/157 - 12s - loss: 0.2499 - accuracy: 0.9052 - val_loss: 0.2632 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 65/100\n","157/157 - 12s - loss: 0.2489 - accuracy: 0.9060 - val_loss: 0.2639 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 66/100\n","157/157 - 12s - loss: 0.2481 - accuracy: 0.9072 - val_loss: 0.2642 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 67/100\n","157/157 - 12s - loss: 0.2472 - accuracy: 0.9080 - val_loss: 0.2645 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 68/100\n","157/157 - 12s - loss: 0.2463 - accuracy: 0.9064 - val_loss: 0.2648 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 69/100\n","157/157 - 12s - loss: 0.2454 - accuracy: 0.9076 - val_loss: 0.2656 - val_accuracy: 0.8810 - 12s/epoch - 78ms/step\n","Epoch 70/100\n","157/157 - 12s - loss: 0.2445 - accuracy: 0.9060 - val_loss: 0.2657 - val_accuracy: 0.8810 - 12s/epoch - 78ms/step\n","Epoch 71/100\n","157/157 - 13s - loss: 0.2435 - accuracy: 0.9080 - val_loss: 0.2659 - val_accuracy: 0.8810 - 13s/epoch - 80ms/step\n","Epoch 72/100\n","157/157 - 13s - loss: 0.2427 - accuracy: 0.9064 - val_loss: 0.2667 - val_accuracy: 0.8810 - 13s/epoch - 80ms/step\n","Epoch 73/100\n","157/157 - 13s - loss: 0.2422 - accuracy: 0.9088 - val_loss: 0.2666 - val_accuracy: 0.8810 - 13s/epoch - 83ms/step\n","Epoch 74/100\n","157/157 - 13s - loss: 0.2410 - accuracy: 0.9068 - val_loss: 0.2669 - val_accuracy: 0.8810 - 13s/epoch - 82ms/step\n","Epoch 75/100\n","157/157 - 13s - loss: 0.2404 - accuracy: 0.9076 - val_loss: 0.2667 - val_accuracy: 0.8810 - 13s/epoch - 81ms/step\n","Epoch 76/100\n","157/157 - 13s - loss: 0.2395 - accuracy: 0.9096 - val_loss: 0.2669 - val_accuracy: 0.8810 - 13s/epoch - 80ms/step\n","Epoch 77/100\n","157/157 - 13s - loss: 0.2386 - accuracy: 0.9088 - val_loss: 0.2672 - val_accuracy: 0.8810 - 13s/epoch - 81ms/step\n","Epoch 78/100\n","157/157 - 13s - loss: 0.2380 - accuracy: 0.9092 - val_loss: 0.2671 - val_accuracy: 0.8810 - 13s/epoch - 80ms/step\n","Epoch 79/100\n","157/157 - 13s - loss: 0.2369 - accuracy: 0.9084 - val_loss: 0.2677 - val_accuracy: 0.8810 - 13s/epoch - 81ms/step\n","Epoch 80/100\n","157/157 - 12s - loss: 0.2363 - accuracy: 0.9100 - val_loss: 0.2680 - val_accuracy: 0.8810 - 12s/epoch - 78ms/step\n","Epoch 81/100\n","157/157 - 12s - loss: 0.2358 - accuracy: 0.9080 - val_loss: 0.2682 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 82/100\n","157/157 - 12s - loss: 0.2349 - accuracy: 0.9104 - val_loss: 0.2685 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 83/100\n","157/157 - 12s - loss: 0.2340 - accuracy: 0.9108 - val_loss: 0.2687 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 84/100\n","157/157 - 12s - loss: 0.2333 - accuracy: 0.9108 - val_loss: 0.2694 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 85/100\n","157/157 - 12s - loss: 0.2331 - accuracy: 0.9092 - val_loss: 0.2693 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 86/100\n","157/157 - 12s - loss: 0.2321 - accuracy: 0.9112 - val_loss: 0.2699 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 87/100\n","157/157 - 12s - loss: 0.2316 - accuracy: 0.9104 - val_loss: 0.2702 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 88/100\n","157/157 - 12s - loss: 0.2310 - accuracy: 0.9104 - val_loss: 0.2703 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 89/100\n","157/157 - 12s - loss: 0.2303 - accuracy: 0.9120 - val_loss: 0.2706 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 90/100\n","157/157 - 12s - loss: 0.2297 - accuracy: 0.9104 - val_loss: 0.2708 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 91/100\n","157/157 - 12s - loss: 0.2290 - accuracy: 0.9108 - val_loss: 0.2714 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 92/100\n","157/157 - 12s - loss: 0.2285 - accuracy: 0.9112 - val_loss: 0.2721 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 93/100\n","157/157 - 12s - loss: 0.2279 - accuracy: 0.9112 - val_loss: 0.2724 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 94/100\n","157/157 - 12s - loss: 0.2275 - accuracy: 0.9108 - val_loss: 0.2722 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 95/100\n","157/157 - 12s - loss: 0.2266 - accuracy: 0.9132 - val_loss: 0.2726 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 96/100\n","157/157 - 12s - loss: 0.2262 - accuracy: 0.9124 - val_loss: 0.2729 - val_accuracy: 0.8810 - 12s/epoch - 75ms/step\n","Epoch 97/100\n","157/157 - 12s - loss: 0.2257 - accuracy: 0.9104 - val_loss: 0.2731 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 98/100\n","157/157 - 12s - loss: 0.2252 - accuracy: 0.9108 - val_loss: 0.2727 - val_accuracy: 0.8810 - 12s/epoch - 76ms/step\n","Epoch 99/100\n","157/157 - 12s - loss: 0.2245 - accuracy: 0.9144 - val_loss: 0.2734 - val_accuracy: 0.8810 - 12s/epoch - 77ms/step\n","Epoch 100/100\n","157/157 - 12s - loss: 0.2237 - accuracy: 0.9112 - val_loss: 0.2738 - val_accuracy: 0.8810 - 12s/epoch - 80ms/step\n","3/3 [==============================] - 0s 70ms/step - loss: 0.2738 - accuracy: 0.8810\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.88      0.89        24\n","           1       0.84      0.89      0.86        18\n","\n","    accuracy                           0.88        42\n","   macro avg       0.88      0.88      0.88        42\n","weighted avg       0.88      0.88      0.88        42\n","\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\big_j\\AppData\\Local\\Temp/ipykernel_8084/3238450248.py:28: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.median(level=1) should use df.groupby(level=1).median().\n","  df.mean(level=0)\n"]},{"ename":"ValueError","evalue":"No model config found in the file at vgg19_Augmentation_pipe_pipeline.h5.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8084/3238450248.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcr_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8084/9660516.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     def transfer_learning(self,\n","\u001b[1;32m~\\anaconda3\\envs\\ecpython\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\envs\\ecpython\\lib\\site-packages\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'No model config found in the file at {filepath}.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'decode'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: No model config found in the file at vgg19_Augmentation_pipe_pipeline.h5."]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["import pandas as pd\n","df = pd.DataFrame\n","cr_list = []\n","cm_list = []\n","# callbacks = [earlystopper]\n","callbacks = [model_checkpoint_callback]\n","# callbacks = []\n","for i in range(1):\n","    # model.get_model()\n","    model.load_model(model_path)\n","    # Train\n","    # model.train(callbacks=callbacks, verbose=2, wb=True)\n","\n","    model.transfer_learning(callbacks=callbacks, verbose=2, wb=True)\n","\n","    # model.model.summary()\n","    cm_, cr_ = crcm(model, X_val, y_val, p=True)\n","    cm_list.append(cm_)\n","    cr_list.append(cr_)\n","    cr_['pos'] = {'precision': i,\n","                'recall': i,\n","                'f1-score': i,\n","                'support': i}\n","    if i > 0:\n","        df = df.append(pd.DataFrame.from_dict(cr_))\n","    else:\n","        df = pd.DataFrame.from_dict(cr_)\n","df.mean(level=0)\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.88      0.97      0.92        30\n","           1       0.89      0.67      0.76        12\n","\n","    accuracy                           0.88        42\n","   macro avg       0.88      0.82      0.84        42\n","weighted avg       0.88      0.88      0.88        42\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["\n","model.model.load_weights(model_filename)\n","_, _ = crcm(model, X_val, y_val, p=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvePI4OaCOKI"},"outputs":[],"source":["if RunningInCOLAB:\n","  from google.colab import drive\n","  drive.mount('/content/drive/')\n","\n","  !cp '{model_filename}' \"/content/drive/MyDrive/model\""]},{"cell_type":"markdown","metadata":{"id":"DdLghhBaScNK"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Baseline_work.ipynb","provenance":[{"file_id":"1D89HMSlvZ2innh0IWbNI_5LYCgG44N1H","timestamp":1645542629687},{"file_id":"https://github.com/wandb/examples/blob/master/colabs/keras/Keras_pipeline_with_Weights_and_Biases.ipynb","timestamp":1645008414025}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}
