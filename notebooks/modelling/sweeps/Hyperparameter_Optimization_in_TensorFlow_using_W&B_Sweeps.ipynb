{"cells":[{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# Setup if running in colab\n","RunningInCOLAB = 'google.colab' in str(get_ipython())\n","if RunningInCOLAB:\n","  try:\n","    if runonce:\n","      print(\"Already ran\")\n","  \n","  except:\n","    runonce = True\n","    !pip install wandb\n","    !git clone https://github.com/Jimmy-Nnilsson/StudieGrupp3_MLProjekt.git\n","    \n","    import wandb\n","    wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuJ29hLgFeGb"},"outputs":[],"source":["import tqdm\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.preprocessing.image import load_img, img_to_array\n","\n","import os\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEjzTuEvFeGc"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","from wandb.keras import WandbCallback\n","\n","wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get base project directory\n","if not RunningInCOLAB:\n","#   project_path = Path(os.getcwd()).parent.parent\n","\n","  for i, p in enumerate(Path(os.getcwd()).parts):\n","    if p == \"StudieGrupp3_MLProjekt\":\n","        break\n","    pathparts = list(Path(os.getcwd()).parts[0:i+2])\n","    project_path = Path(pathparts[0],\"\\\\\\\\\".join(pathparts[1:]))\n","else:\n","  project_path = Path('/content/StudieGrupp3_MLProjekt/')\n","datapath = (project_path /'data/processed/')\n","\n","CLASSES = {0 : 'yes', 1 : 'no'}\n","# Loops through pathlist and reads and resizes images\n","def read_image(pathlist : list, size : int)-> list:\n","    data = []\n","    for path in pathlist:\n","        image=load_img(path, color_mode='rgb', target_size=(size, size))\n","        # image=load_img(path, color_mode='rgb', target_size=(size, size))\n","        image=img_to_array(image)\n","        # image=image/255.0\n","        data.append(image)\n","    data = np.asarray(data, dtype=np.uint8)\n","    return data\n","\n","# Makes input and label data from folder locations.\n","# Loops through location \"subfolder/CLASSES\"\n","def get_sets(subfolder : str, CLASSES : dict, size : int):\n","    folder_paths = []\n","    folder_labels = []\n","    labels = []\n","    for k,v in CLASSES.items():\n","        # input datapath generation\n","        folder_paths += list((datapath / f\"2_split_{v}/{subfolder}\").rglob(\"*\"))\n","    # Label data generation\n","    folder_labels = [0 if x.stem.split('_')[1] == 'yes' else 1 for x in folder_paths]\n","    folder_labels = np.asarray(folder_labels, dtype=np.uint8)\n","    # Extract images from datapaths\n","    img_list = read_image(folder_paths, size)\n","\n","    return img_list, folder_labels\n","\n","def get_training_set(CLASSES : dict, size : int):\n","    folder_paths = []\n","    folder_labels = []\n","    labels = []\n","    for k,v in CLASSES.items():\n","        # input datapath generation\n","        folder_paths += list((datapath / f\"3_aug_{v}_train/\").rglob(\"*\"))\n","        # folder_paths += list((datapath / f\"3_augmentation_train/3_aug_geo_{v}_train/\").rglob(\"*\"))\n","        # folder_paths += list((datapath / f\"3_augmentation_train/3_aug_pix_{v}_train/\").rglob(\"*\"))\n","        # print(folder_paths)\n","    # Label data generation\n","    folder_labels = [0 if x.stem.split('_')[1] == 'yes' else 1 for x in folder_paths]\n","    # Extract images from datapaths\n","    img_list = read_image(folder_paths, size)\n","\n","    return img_list, folder_labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dataset inspect\n","# Read images to variables\n","size = 224\n","X_aug_train, y_aug_train = get_training_set(CLASSES, size)\n","X_train, y_train = get_sets('train', CLASSES, size)\n","X_val, y_val = get_sets('val', CLASSES, size)\n","X_test, y_test = get_sets('test', CLASSES, size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Mind model processing\n","# Finetune not complete\n","configs = dict(\n","    project_name = \"s2\", #Project Name\n","    mode = 'run', #{'offline', 'run', 'disabled', 'dryrun', 'online'} # WandB run status\n","    job_type = \"\", #Run type for WandB\n","    group = \"\", # Group in WandB\n","    sub_group = \"_aug_geo_pix\",\n","\n","    class_names = CLASSES, # Classes for training\n","    training_set = (keras.applications.vgg19.preprocess_input(X_aug_train), y_aug_train),\n","\n","    image_width = X_train[0].shape[0], # Picture width for model input\n","    image_height = X_train[0].shape[1], # Picture height for model input\n","    image_channels = X_train[0].shape[2], # Picture channels for model input\n","\n","    pretrain_weights = 'imagenet', # pretrained weights for basemodel if any\n","    batch_size = 4, # Batchsize for training\n","    init_learning_rate = 0.001, # Initial training rate if no callback is used\n","    lr_decay_rate = 0.1, #decayrate of training rate\n","    epochs = 50, # Epochs to train\n","    optimizer = 'adam', # The optimizer used by the ml model\n","    loss_fn = 'binary_crossentropy', # Loss function\n","    metrics = ['accuracy'], # Metrics\n","    earlystopping_patience = 5, # For the early stopping callback\n","\n","    dataset = \"Brain_MRI_Images_for_Brain_Tumor_Detection\",\n","    fine_tune_learning_rate = 1e-5, # learningrate Used during fine tuning\n","    fine_tune_epochs = 10, # Epochs ran at finetuning\n","\n","    architecture = \"\",# To be defined f\"{base_model._name.upper()} global_average_pooling2d\",\n","    model_name = '' # set after model is defined # Name of the ml Model\n","\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_config = {\n","    'method': \"random\",\n","    'metric': {\n","        'name': 'accuracy',\n","        'goal': 'maximize',\n","    },\n","    'parameters': {\n","        \"optimizer\": {\n","            \"values\": ['adam', 'sgd', 'rmsprop']\n","        },\n","        \"nodes\": {\n","            \"values\": [128, 256, 512]\n","        },\n","        \"epochs\": {\n","            \"values\": [1,2,3,5]\n","        },\n","        \"learning_rate\": {\n","            \"distribution\": \"uniform\",\n","            \"min\": 0.0001,\n","            \"max\": 0.1\n","        },\n","        \"batch_size\": {\n","            \"distribution\": \"q_log_uniform\",\n","            \"q\": 1,\n","            \"min\": 32,\n","            \"max\": 128\n","        },\n","    },\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xno5YgDxFeGg"},"outputs":[],"source":["sweep_config = {\n","  'method': 'random', \n","  'metric': {\n","      'name': 'val_loss',\n","      'goal': 'minimize'\n","  },\n","  'early_terminate':{\n","      'type': 'hyperband',\n","      'min_iter': 5\n","  },\n","  'parameters': {\n","        \"optimizer\": {\n","            \"values\": ['adam', 'sgd', 'rmsprop']\n","        },\n","      'batch_size': {\n","          'values': [32, 64, 128]\n","      },\n","      'learning_rate':{\n","          'values': [0.01, 0.005, 0.001, 0.0005, 0.0001]\n","      },\n","      'model_bridge':{\n","          'values': ['flatten', 'gap']\n","      },\n","      'layers':{\n","          'values': [0, 1, 2, 3]\n","      },\n","      'nodes':{\n","          'values': [32, 64, 128]\n","      }\n","  }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def Model(bridge, layers, nodes):\n","    kwarg = dict(weights=configs['pretrain_weights'], include_top=False, input_shape=(configs['image_width'],configs['image_height'],configs['image_channels']))\n","    base_model = keras.applications.vgg19.VGG19(**kwarg)\n","\n","    base_model.trainable = False\n","    # bridge between basemodel and endstuff\n","    if bridge == 'gap':\n","        x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n","    elif bridge == 'flatten':\n","        x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n","\n","    if layers > 0:\n","        for i in range(layers):\n","            x = keras.layers.Dense(int(nodes/(i+1)), activation='relu')(x)\n","\n","    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n","    configs['group'] = f'{base_model._name}{configs[\"sub_group\"]}'\n","    configs['architecture'] = base_model._name\n","    model  = keras.Model(base_model.input, outputs, name=f'Baseline_{base_model._name.upper()}')\n","\n","    return model\n","\n","    \n","def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):\n","    with tf.GradientTape() as tape:\n","        logits = model(x, training=True)\n","        loss_value = loss_fn(y, logits)\n","\n","    grads = tape.gradient(loss_value, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","    train_acc_metric.update_state(y, logits)\n","\n","    return loss_value\n","\n","    \n","def test_step(x, y, model, loss_fn, val_acc_metric):\n","    val_logits = model(x, training=False)\n","    loss_value = loss_fn(y, val_logits)\n","    val_acc_metric.update_state(y, val_logits)\n","\n","    return loss_value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kX8NiGBwFeGf"},"outputs":[],"source":["def train(train_dataset,\n","          val_dataset, \n","          model,\n","          optimizer,\n","          loss_fn,\n","          train_acc_metric,\n","          val_acc_metric,\n","          epochs=10, \n","          log_step=200, \n","          val_log_step=50):\n","  \n","    for epoch in range(epochs):\n","        print(\"\\nStart of epoch %d\" % (epoch,))\n","\n","        train_loss = []\n","        val_loss = []\n","\n","        # Iterate over the batches of the dataset\n","        for step, (x_batch_train, y_batch_train) in tqdm.tqdm(enumerate(train_dataset), total=len(train_dataset)):\n","            loss_value = train_step(x_batch_train, y_batch_train, \n","                                    model, optimizer, \n","                                    loss_fn, train_acc_metric)\n","            train_loss.append(float(loss_value))\n","\n","        # Run a validation loop at the end of each epoch\n","        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n","            val_loss_value = test_step(x_batch_val, y_batch_val, \n","                                       model, loss_fn, \n","                                       val_acc_metric)\n","            val_loss.append(float(val_loss_value))\n","\n","        # Display metrics at the end of each epoch\n","        train_acc = train_acc_metric.result()\n","        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n","\n","        val_acc = val_acc_metric.result()\n","        print(\"Validation acc: %.4f\" % (float(val_acc),))\n","\n","        # Reset metrics at the end of each epoch\n","        train_acc_metric.reset_states()\n","        val_acc_metric.reset_states()\n","\n","        # 3️⃣ log metrics using wandb.log\n","        wandb.log({'epochs': epoch,\n","                   'loss': np.mean(train_loss),\n","                   'acc': float(train_acc), \n","                   'val_loss': np.mean(val_loss),\n","                   'val_acc':float(val_acc)})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZFhIcJkFeGg"},"outputs":[],"source":["def sweep_train(config_defaults=None):\n","    # Set default values\n","    config_defaults = {\n","        \"batch_size\": 64,\n","        \"learning_rate\": 0.01\n","    }\n","    # Initialize wandb with a sample project name\n","    wandb.init(config=config_defaults)  # this gets over-written in the Sweep\n","\n","    # Specify the other hyperparameters to the configuration, if any\n","    wandb.config.epochs = 2\n","    wandb.config.log_step = 20\n","    wandb.config.val_log_step = 50\n","    wandb.config.architecture_name = configs['model_name']\n","    wandb.config.dataset_name = configs['dataset']\n","\n","    # build input pipeline using tf.data\n","    train_dataset = tf.data.Dataset.from_tensor_slices(configs['training_set'])\n","    train_dataset = (train_dataset.shuffle(buffer_size=512)\n","                                  .batch(wandb.config.batch_size)\n","                                  .prefetch(buffer_size=tf.data.AUTOTUNE))\n","\n","    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n","    val_dataset = (val_dataset.batch(wandb.config.batch_size)\n","                              .prefetch(buffer_size=tf.data.AUTOTUNE))\n","\n","    # initialize model\n","    model = Model(wandb.config.model_bridge, wandb.config.layers, wandb.config.nodes)\n","\n","    # Instantiate an optimizer to train the model.\n","    if wandb.config.optimizer == \"sgd\":\n","        optimizer = keras.optimizers.SGD(learning_rate=wandb.config.learning_rate)\n","    elif wandb.config.optimizer == \"rmsprop\":\n","        optimizer = keras.optimizers.RMSprop(learning_rate=wandb.config.learning_rate)\n","    elif wandb.config.optimizer == \"adam\":\n","        optimizer = keras.optimizers.Adam(learning_rate=wandb.config.learning_rate)\n","\n","    # Instantiate a loss function.\n","    loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","    # Prepare the metrics.\n","    train_acc_metric = keras.metrics.BinaryAccuracy()\n","    val_acc_metric = keras.metrics.BinaryAccuracy()\n","\n","    train(train_dataset,\n","          val_dataset, \n","          model,\n","          optimizer,\n","          loss_fn,\n","          train_acc_metric,\n","          val_acc_metric,\n","          epochs=wandb.config.epochs, \n","          log_step=wandb.config.log_step, \n","          val_log_step=wandb.config.val_log_step)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-GPPyrqFeGh"},"outputs":[{"name":"stdout","output_type":"stream","text":["Create sweep with ID: bs8wai5f\n","Sweep URL: https://wandb.ai/bex_team/s2/sweeps/bs8wai5f\n"]}],"source":["sweep_id = wandb.sweep(sweep_config, project=configs['project_name'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIETIYF0FeGh"},"outputs":[],"source":["# wandb.agent(sweep_id, function=sweep_train, count=10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5qiczmgq with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_bridge: flatten\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnodes: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/bex_team/s2/runs/5qiczmgq\" target=\"_blank\">winter-sweep-48</a></strong> to <a href=\"https://wandb.ai/bex_team/s2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","Sweep page: <a href=\"https://wandb.ai/bex_team/s2/sweeps/pc0g0ncm\" target=\"_blank\">https://wandb.ai/bex_team/s2/sweeps/pc0g0ncm</a><br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Start of epoch 0\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:40<00:00,  1.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training acc over epoch: 0.6260\n","Validation acc: 0.3750\n","\n","Start of epoch 1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 63/63 [00:38<00:00,  1.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training acc over epoch: 0.6295\n","Validation acc: 0.3750\n"]},{"data":{"text/html":["<br/>Waiting for W&B process to finish, PID 7028... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\">\n","<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁█</td></tr><tr><td>epochs</td><td>▁█</td></tr><tr><td>loss</td><td>█▁</td></tr><tr><td>val_acc</td><td>▁▁</td></tr><tr><td>val_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\">\n","<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.62946</td></tr><tr><td>epochs</td><td>1</td></tr><tr><td>loss</td><td>0.69317</td></tr><tr><td>val_acc</td><td>0.375</td></tr><tr><td>val_loss</td><td>0.69294</td></tr></table>\n","</div></div>\n","Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","<br/>Synced <strong style=\"color:#cdcd00\">winter-sweep-48</strong>: <a href=\"https://wandb.ai/bex_team/s2/runs/5qiczmgq\" target=\"_blank\">https://wandb.ai/bex_team/s2/runs/5qiczmgq</a><br/>\n","Find logs at: <code>.\\wandb\\run-20220226_182942-5qiczmgq\\logs</code><br/>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# sweep_id = wandb.sweep(sweep_config, project=configs['project_name'])\n","sweep_id = \"pc0g0ncm\"\n","\n","\n","# wandb.agent(sweep_id, function=sweep_train, count=10)\n","\n","wandb.agent(\n","    sweep_id, function=sweep_train, entity=\"bex_team\", project=configs['project_name'], count=1\n",")"]}],"metadata":{"colab":{"name":"Copy of Hyperparameter_Optimization_in_TensorFlow_using_W&B_Sweeps.ipynb","provenance":[{"file_id":"https://github.com/wandb/examples/blob/master/colabs/tensorflow/Hyperparameter_Optimization_in_TensorFlow_using_W%26B_Sweeps.ipynb","timestamp":1645846389613}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}
